{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Regression\n",
        "\n",
        "Random Forest Regression is an ensemble learning technique that combines multiple decision tree regressors to improve prediction accuracy and control overfitting. Here’s an in-depth look at how it works:\n",
        "\n",
        "### Basic Concept\n",
        "\n",
        "A random forest is composed of a collection of decision tree regressors, where each tree is trained on a random subset of the data and a random subset of the features. The final prediction is made by averaging the predictions of all individual trees.\n",
        "\n",
        "### Steps Involved\n",
        "\n",
        "#### 1. Bootstrapping\n",
        "\n",
        "The first step in building a random forest is to create multiple subsets of the original dataset using a method called bootstrapping. In bootstrapping:\n",
        "- **Random Sampling with Replacement**: A subset of the original dataset is created by randomly selecting samples with replacement. This means some samples may appear multiple times, and some may not appear at all.\n",
        "\n",
        "#### 2. Building Decision Trees\n",
        "\n",
        "Each bootstrapped subset is used to train a separate decision tree. During the training of each tree:\n",
        "- **Random Feature Selection**: At each split in the tree, a random subset of features is considered rather than evaluating all features. This introduces more diversity among the trees.\n",
        "\n",
        "#### 3. Aggregating Predictions\n",
        "\n",
        "Once all trees are trained, predictions are made for each tree, and the final prediction for the random forest is obtained by averaging the predictions of all trees.\n",
        "\n",
        "### Detailed Process\n",
        "\n",
        "#### 1. Data Preparation\n",
        "- **Original Dataset**: Assume we have a dataset $D$ with $N$ samples and $M$ features.\n",
        "\n",
        "#### 2. Bootstrapping\n",
        "- **Creating Bootstrapped Subsets**: For $B$ trees, create $B$ bootstrapped subsets $D_1, D_2, \\ldots, D_B$, each containing $N$ samples randomly chosen with replacement from $D$.\n",
        "\n",
        "#### 3. Building Trees\n",
        "- **Training Individual Trees**: For each bootstrapped subset $D_b$:\n",
        "  - Start with a root node.\n",
        "  - For each node in the tree, select a random subset of $k$ features (where $k \\leq M$).\n",
        "  - Find the best split among the selected features using criteria like Mean Squared Error (MSE).\n",
        "  - Split the node based on the best feature and repeat the process recursively for child nodes until stopping criteria are met (e.g., maximum depth, minimum samples per leaf).\n",
        "  - Each tree $T_b$ is grown to full depth or until it meets other stopping criteria.\n",
        "\n",
        "#### 4. Making Predictions\n",
        "- **Individual Tree Prediction**: For a new data point $x$, each tree $T_b$ makes a prediction $\\hat{y}_b(x)$.\n",
        "- **Aggregating Predictions**: The final prediction $\\hat{y}(x)$ of the random forest is the average of the predictions of all trees:\n",
        "  $ \\hat{y}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{y}_b(x) $\n",
        "\n",
        "### Advantages of Random Forest Regression\n",
        "\n",
        "1. **Improved Accuracy**: By averaging the predictions of multiple trees, random forests reduce the risk of overfitting and generally provide more accurate predictions compared to individual decision trees.\n",
        "2. **Robustness**: Random forests are less sensitive to outliers and noise in the data due to the averaging process.\n",
        "3. **Feature Importance**: They provide a measure of feature importance, which helps in understanding the contribution of each feature to the prediction.\n",
        "\n",
        "### Disadvantages of Random Forest Regression\n",
        "\n",
        "1. **Complexity**: Random forests are more complex and require more computational resources than individual decision trees.\n",
        "2. **Interpretability**: The ensemble nature makes them harder to interpret compared to a single decision tree.\n",
        "3. **Training Time**: Training multiple trees can be time-consuming, especially with large datasets.\n",
        "\n",
        "### Example Scenario\n",
        "\n",
        "Imagine we want to predict the price of houses based on various features such as size, location, number of rooms, and age of the house. Here's how a random forest regression model would work:\n",
        "\n",
        "1. **Bootstrapping**: Create multiple bootstrapped subsets of the house dataset.\n",
        "2. **Building Trees**: Train a decision tree on each subset, randomly selecting a subset of features at each split.\n",
        "3. **Aggregating Predictions**: For a new house, each tree in the forest makes a price prediction, and the final predicted price is the average of these predictions.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "#### 1. Bootstrapping\n",
        "\n",
        "Given a dataset $D$ with $N$ samples, create $B$ bootstrapped datasets $D_1, D_2, \\ldots, D_B$, each containing $N$ samples randomly selected with replacement from $D$.\n",
        "\n",
        "#### 2. Random Feature Selection\n",
        "\n",
        "For each tree $T_b$:\n",
        "- At each split, select a random subset of $k$ features from the $M$ available features.\n",
        "- Evaluate the best split among the $k$ features based on criteria such as MSE.\n",
        "\n",
        "#### 3. Tree Prediction\n",
        "\n",
        "For a new input $x$, each tree $T_b$ makes a prediction $\\hat{y}_b(x)$.\n",
        "\n",
        "#### 4. Aggregated Prediction\n",
        "\n",
        "The final prediction $\\hat{y}(x)$ of the random forest is:\n",
        "$ \\hat{y}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{y}_b(x) $\n",
        "\n",
        "In summary, Random Forest Regression leverages the power of multiple decision trees, trained on random subsets of the data and features, to provide more robust and accurate predictions. It balances the bias-variance tradeoff by reducing overfitting while maintaining high predictive performance."
      ],
      "metadata": {
        "id": "ggeNBWuBE3Uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "XC4pJ99TI2US"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LeFFGHHSEwgJ"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Data\n",
        "\n",
        "data = pd.read_csv('Position_Salaries.csv')\n",
        "X = data.iloc[:, 1:-1].values\n",
        "y = data.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "CBS2A0-PJABF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Random Forest Regression model on the whole data"
      ],
      "metadata": {
        "id": "oPxGEIa2Jd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
        "regressor.fit(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "EtU1ZMs-JNX1",
        "outputId": "f339747e-3cfc-499b-f5e6-d8a4e4366f6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(n_estimators=10, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=10, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Results"
      ],
      "metadata": {
        "id": "Yknt0cIeKitj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.predict([[6.5]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHRaRlB3KHZ_",
        "outputId": "6b4792bf-00df-4396-9ff2-ff963bb99f66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([167000.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}